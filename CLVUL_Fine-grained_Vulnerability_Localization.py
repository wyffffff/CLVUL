import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from transformers import RobertaModel, RobertaTokenizer
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score
import os

###########################
# 1. 设置随机种子和配置   #
###########################

def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True

set_seed(42)

MODEL_NAME = "unixcoder-base-nine"
BATCH_SIZE = 128
LEARNING_RATE = 1e-5
NUM_EPOCHS = 1
MAX_LENGTH = 128
TEMPERATURE = 0.1

MODEL_SAVE_DIR = 'saved_models_lineCL'
MODEL_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, 'best_model_line.pth')

LABEL_DESCRIPTIONS = {
    0: "This line does not contain defects.",
    1: "This line contains defects."
}

###########################
# 2. 数据集（行级）定义   #
###########################

class LineDefectDataset(Dataset):
    """
    将函数源码按行拆分，每行做一个训练样本。
    同时保留:
      - func_id: 函数编号(方便后续聚合行做行内评估)
      - line_number: 行号(0-based)
    label_id: 行是否缺陷(0/1)
    """
    def __init__(self, dataframe, tokenizer, max_length):
        """
        假设 dataframe 至少包含:
          - 'processed_func': str，函数的完整源码(多行)
          - 'flaw_line_index': str，逗号分隔的缺陷行号(1-based)
        """
        self.samples = []  # 每个元素是一个dict, 包含行文本、标签、func_id、line_number

        for func_id, row in dataframe.iterrows():
            code_str = str(row.get('processed_func', ''))
            flaw_line_str = row.get('flaw_line_index', '')
            flaw_indices = []
            if isinstance(flaw_line_str, str) and flaw_line_str.strip():
                flaw_indices = [int(x.strip()) - 1 for x in flaw_line_str.split(',') if x.strip().isdigit()]
            lines = code_str.split('\n')
            for i, line_text in enumerate(lines):
                label = 1 if i in flaw_indices else 0
                self.samples.append({
                    'func_id': func_id,
                    'line_number': i,
                    'line_text': line_text.strip(),
                    'label_id': label
                })

        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        record = self.samples[idx]
        line_text = record['line_text']

        encoding = self.tokenizer(
            line_text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'func_id': record['func_id'],
            'line_number': record['line_number'],
            'code_input_ids': encoding['input_ids'].squeeze(0),
            'code_attention_mask': encoding['attention_mask'].squeeze(0),
            'label_id': record['label_id']
        }

###########################
# 3. 模型定义 (行级CLIP)  #
###########################

class CodeTextCLIPModel(nn.Module):
    """
    用一个 Roberta 编码器获取行文本的向量表示；
    同时用 text_encoder 初始化 2 个标签嵌入(无缺陷、有缺陷)。
    """
    def __init__(self, model_name, num_classes, temperature):
        super(CodeTextCLIPModel, self).__init__()
        self.code_encoder = RobertaModel.from_pretrained(model_name)
        self.text_encoder = RobertaModel.from_pretrained(model_name)
        self.temperature = temperature

        # 初始化 2 个标签嵌入
        label_texts = [LABEL_DESCRIPTIONS[i] for i in range(num_classes)]
        tokenizer = RobertaTokenizer.from_pretrained(model_name)
        label_enc = tokenizer(
            label_texts,
            add_special_tokens=True,
            max_length=MAX_LENGTH // 4,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        with torch.no_grad():
            outputs = self.text_encoder(
                input_ids=label_enc['input_ids'],
                attention_mask=label_enc['attention_mask']
            )
            label_emb = outputs.last_hidden_state[:, 0, :]
            label_emb = nn.functional.normalize(label_emb, dim=1)

        # 将 label_embeddings 设置为可训练参数
        self.label_embeddings = nn.Parameter(label_emb)

    def forward(self, code_input_ids, code_attention_mask):
        outputs = self.code_encoder(
            input_ids=code_input_ids,
            attention_mask=code_attention_mask
        )
        # 取 [CLS] 向量
        code_emb = outputs.last_hidden_state[:, 0, :]
        code_emb = nn.functional.normalize(code_emb, dim=1)
        return code_emb

###########################
# 4. 损失函数 (对比学习)  #
###########################

class LabelContrastiveLoss(nn.Module):
    """
    与原先相同，对 (行向量) 与 (label_embeddings) 做对比后计算交叉熵。
    """
    def __init__(self, temperature=0.1):
        super(LabelContrastiveLoss, self).__init__()
        self.temperature = temperature
        self.cross_entropy = nn.CrossEntropyLoss()

    def forward(self, code_embeddings, label_embeddings, labels):
        """
        code_embeddings: (batch_size, hidden_dim)
        label_embeddings: (num_classes, hidden_dim)
        labels: (batch_size,) in {0,1}
        """
        logits = torch.matmul(code_embeddings, label_embeddings.t()) / self.temperature
        loss = self.cross_entropy(logits, labels)
        return loss

###########################
# 5. 数据加载与拆分       #
###########################

def load_data(csv_path, tokenizer, max_length):
    """
    读取 CSV 并拆分为 train/val/test (8:1:1)。
    在这里增加过滤，只保留第一列值为 1 的样本。
    返回 DataLoader 和原始 dataframe (仅为方便函数编号).
    """
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"数据文件 '{csv_path}' 不存在。")

    df = pd.read_csv(csv_path).reset_index(drop=True)
    
    # 只保留第一列值为 1 的样本（假设第一列为目标列）
    df = df[df.iloc[:, 0] == 1].reset_index(drop=True)
    
    dataset = LineDefectDataset(df, tokenizer, max_length)

    total_size = len(dataset)
    train_size = int(0.8 * total_size)
    val_size = int(0.1 * total_size)
    test_size = total_size - train_size - val_size

    train_ds, val_ds, test_ds = random_split(
        dataset, [train_size, val_size, test_size],
        generator=torch.Generator().manual_seed(42)
    )

    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)

    return df, train_loader, val_loader, test_loader

###########################
# 6. 训练函数             #
###########################

def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):
    best_accuracy = 0.0

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0.0
        pbar = tqdm(train_loader, desc=f"训练 第{epoch+1}/{num_epochs}轮", leave=False)

        for batch in pbar:
            code_input_ids = batch['code_input_ids'].to(device)
            code_attention_mask = batch['code_attention_mask'].to(device)
            labels = batch['label_id'].to(device)

            code_emb = model(code_input_ids, code_attention_mask)
            label_emb = model.label_embeddings  # [2, hidden_dim]

            loss = criterion(code_emb, label_emb, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            avg_loss = total_loss / (pbar.n + 1)
            pbar.set_postfix({'loss': f"{avg_loss:.4f}"})

        print(f"[Epoch {epoch+1}/{num_epochs}] Loss: {avg_loss:.4f}")

        # 验证集行级准确率
        accuracy = evaluate_model(model, val_loader, device, mode='validation')
        print(f"[Epoch {epoch+1}] Val Accuracy: {accuracy:.4f}")

        # 保存最佳
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            torch.save(model.state_dict(), MODEL_SAVE_PATH)
            print(f"[*] Best model saved (Val Acc = {best_accuracy:.4f}).")

    # 加载最佳模型
    if os.path.exists(MODEL_SAVE_PATH):
        model.load_state_dict(torch.load(MODEL_SAVE_PATH))
        model.to(device)
        print(f"最佳模型已加载，验证集准确率: {best_accuracy:.4f}")
    else:
        print("[WARN] 未找到最佳模型文件。")

###########################
# 7. 常规行级评估         #
###########################

def evaluate_model(model, data_loader, device, mode='test'):
    """
    常规行级二分类评估: 计算Accuracy/F1等。
    mode='test'时输出详细报告，否则只返回accuracy。
    """
    model.eval()
    y_true, y_pred = [], []
    with torch.no_grad():
        label_emb = model.label_embeddings
        label_emb = nn.functional.normalize(label_emb, dim=1)

        pbar = tqdm(data_loader, desc=f"评估 {mode} 集", leave=False)
        for batch in pbar:
            code_input_ids = batch['code_input_ids'].to(device)
            code_attention_mask = batch['code_attention_mask'].to(device)
            labels = batch['label_id'].cpu().numpy()

            code_emb = model(code_input_ids, code_attention_mask)
            logits = torch.matmul(code_emb, label_emb.t()) / model.temperature  # [batch_size, 2]
            preds = torch.argmax(logits, dim=1).cpu().numpy()

            y_true.extend(labels)
            y_pred.extend(preds)

    accuracy = accuracy_score(y_true, y_pred)
    if mode == 'test':
        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)

        print(f"[Test] Accuracy: {accuracy:.4f}")
        print(f"[Test] Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")

        names = [LABEL_DESCRIPTIONS[i] for i in sorted(LABEL_DESCRIPTIONS.keys())]
        report = classification_report(y_true, y_pred, target_names=names, zero_division=0)
        print("分类报告:\n", report)

    return accuracy

###########################
# 8. 行内排序指标 (Top-10, IFA) #
###########################

def line_level_evaluation(line_preds, flaw_line_indices):
    """
    给定某个函数内的所有行预测结果 line_preds, 以及真实缺陷行集合 flaw_line_indices(0-based),
    根据行的相似度或置信度排序, 计算:
      - Top-10 Accuracy: 前10行是否至少含1个真实缺陷行
      - IFA: 找到第一个真实缺陷行前的误报数
    line_preds: [{ 'line_number': int, 'score': float }, ... ]
    """
    # 按分数降序
    sorted_lines = sorted(line_preds, key=lambda x: x['score'], reverse=True)

    top_10 = sorted_lines[:10]
    top_10_idx = [item['line_number'] for item in top_10]
    found_defect = len(set(top_10_idx) & set(flaw_line_indices)) > 0
    top_10_acc = 1.0 if found_defect else 0.0

    ifa = 0
    for item in sorted_lines:
        if item['line_number'] in flaw_line_indices:
            break
        ifa += 1

    return {
        'top_10_accuracy': top_10_acc,
        'ifa': ifa
    }

def line_level_evaluate_model(model, data_loader, df, device):
    """
    对测试集中每个函数内的行进行排序, 并计算Top-10 Accuracy和IFA等指标。
    - 首先推断每一行"有缺陷"标签1的置信度(或相似度).
    - 再按 func_id 分组, 与 flaw_line_index 做比较.
    """
    model.eval()

    # 收集所有行的预测分数
    # key: func_id, value: list of { line_number, score }
    func_line_scores = {}

    # 从原始 df 提取 flaw_line_index, 以便评估
    # key: func_id => set([缺陷行(0-based), ...])
    func_flaw_dict = {}
    for fid, row in df.iterrows():
        flaw_line_str = row.get('flaw_line_index', '')
        flaw_idx = set()
        if isinstance(flaw_line_str, str) and flaw_line_str.strip():
            flaw_idx = set(int(x.strip()) - 1 for x in flaw_line_str.split(',') if x.strip().isdigit())
        func_flaw_dict[fid] = flaw_idx

    # 计算每行对 label=1(有缺陷) 的logit或score
    with torch.no_grad():
        label_emb = model.label_embeddings  # [2, hidden_dim]
        label_emb = nn.functional.normalize(label_emb, dim=1)

        # label=1向量
        defect_vec = label_emb[1]  # [hidden_dim]
        # 使用 logits[:,1] 作为score
        pbar = tqdm(data_loader, desc="行级推断以计算Top-10/IFA", leave=False)

        for batch in pbar:
            func_ids = batch['func_id'].cpu().numpy()
            line_nums = batch['line_number'].cpu().numpy()

            code_input_ids = batch['code_input_ids'].to(device)
            code_attention_mask = batch['code_attention_mask'].to(device)

            code_emb = model(code_input_ids, code_attention_mask)  # [batch_size, hidden_dim]
            logits = torch.matmul(code_emb, label_emb.t()) / model.temperature  # [batch_size, 2]
            # 缺陷类别的 logit(列1)作为score
            defect_scores = logits[:, 1].cpu().numpy()  # [batch_size]

            for i in range(len(func_ids)):
                fid = func_ids[i]
                ln = line_nums[i]
                score_val = defect_scores[i]

                if fid not in func_line_scores:
                    func_line_scores[fid] = []
                func_line_scores[fid].append({
                    'line_number': ln,
                    'score': score_val
                })

    # 计算每个函数的 top-10 accuracy & IFA
    all_top10 = []
    all_ifa = []
    for fid, line_list in func_line_scores.items():
        flaw_lines = func_flaw_dict.get(fid, set())
        if len(flaw_lines) == 0:
            # 如果函数没有缺陷行，则不计算
            continue

        eval_res = line_level_evaluation(line_list, flaw_lines)
        all_top10.append(eval_res['top_10_accuracy'])
        all_ifa.append(eval_res['ifa'])

    if len(all_top10) == 0:
        print("[WARN] 测试集中没有含缺陷行的函数，无法计算Top-10/IFA。")
    else:
        mean_top10 = np.mean(all_top10)
        mean_ifa = np.mean(all_ifa)
        print("======== 行内排序评估结果 ========")
        print(f"Top-10 Accuracy 平均值: {mean_top10:.4f}")
        print(f"IFA 平均值: {mean_ifa:.4f}")

###########################
# 9. 主流程               #
###########################

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("[INFO] Using device:", device)

    # 加载分词器
    tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)

    # 读取数据并拆分，只保留第一列值为 1 的样本
    csv_path = 'bigvul_linevul_11.4.csv'  # 修改为你的CSV文件路径
    df, train_loader, val_loader, test_loader = load_data(csv_path, tokenizer, MAX_LENGTH)
    print(f"[INFO] Dataset行数: train({len(train_loader.dataset)}), val({len(val_loader.dataset)}), test({len(test_loader.dataset)})")

    # 初始化模型
    num_classes = 2
    model = CodeTextCLIPModel(MODEL_NAME, num_classes, TEMPERATURE).to(device)
    criterion = LabelContrastiveLoss(temperature=TEMPERATURE)
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)

    # 若目录不存在则创建
    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

    # 训练并验证(行级)
    train_model(model, train_loader, val_loader, optimizer, criterion, device, NUM_EPOCHS)

    # 在测试集做行级准确率评估
    print("\n[阶段] 测试集行级分类评估:")
    evaluate_model(model, test_loader, device, mode='test')

    # 在测试集上做“行内排序”指标评估(Top-10 Accuracy, IFA)
    print("\n[阶段] 测试集行内排序评估(Top-10 Accuracy, IFA):")
    line_level_evaluate_model(model, test_loader, df, device)

if __name__ == "__main__":
    main()
